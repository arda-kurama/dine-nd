name: daily-update

on:
    schedule:
        - cron: "0 5 * * *" # 1:00 AM ET
    workflow_dispatch:

jobs:
    scrape_and_embed:
        runs-on: ubuntu-latest
        steps:
            - name: Checkout main
              uses: actions/checkout@v3
              with:
                  ref: main
                  fetch-depth: 0

            # Still needed for the CBORD (NetNutrition) scraper, which uses Selenium
            - name: Install Chrome and matching ChromeDriver
              uses: browser-actions/setup-chrome@v2
              with:
                  chrome-version: stable
                  install-dependencies: true
                  install-chromedriver: true

            - name: Install Python 3.12 with pip caching
              uses: actions/setup-python@v4
              with:
                  python-version: "3.12.3"
                  cache: "pip"
                  cache-dependency-path: requirements.txt

            - name: Install Python dependencies
              run: |
                  python -m pip install --upgrade pip
                  pip install -r requirements.txt

            - name: Run Nutrislice + CBORD scrapers and merge JSONs
              run: |
                  set -euo pipefail
                  mkdir -p /tmp

                  echo "Running Nutrislice scraper (North + South)..."
                  python -m nutrislice_scraper.main
                  mv menu_summary.json /tmp/menu_summary_nutri.json
                  mv consolidated_menu.json /tmp/consolidated_menu_nutri.json

                  echo "Running CBORD scraper (Holy Cross + Saint Mary's)..."
                  python -m cbord_scraper.main
                  mv menu_summary.json /tmp/menu_summary_cbord.json
                  mv consolidated_menu.json /tmp/consolidated_menu_cbord.json

                  echo "Merging into final JSONs..."
                  python merge_menus.py                     --cbord /tmp/consolidated_menu_cbord.json                     --nutri /tmp/consolidated_menu_nutri.json                     --out-consolidated /tmp/consolidated_menu.json                     --out-summary /tmp/menu_summary.json

            # Soft check: measure meal count but don't fail yet
            - name: Soft-check meal count
              id: soft_check
              shell: bash
              run: |
                  set -euo pipefail
                  JSON=/tmp/consolidated_menu.json
                  if [[ ! -s "$JSON" ]]; then
                    echo "total_meals=0" >> "$GITHUB_OUTPUT"
                    exit 0
                  fi
                  python - <<'PY'
                  import json, os
                  p = "/tmp/consolidated_menu.json"
                  with open(p, "r", encoding="utf-8") as f:
                      data = json.load(f)
                  dh = data.get("dining_halls", {})
                  total_meals = sum(len(meals) for meals in dh.values() if isinstance(meals, dict)) if isinstance(dh, dict) else 0
                  with open(os.environ["GITHUB_OUTPUT"], "a") as out:
                      out.write(f"total_meals={total_meals}\n")
                  print(f"Soft check: total_meals={total_meals}")
                  PY

            # If soft check found 0 meals, wait and retry a full run (both scrapers + merge)
            - name: Retry scrape due to empty menu (soft-check triggered)
              if: steps.soft_check.outputs.total_meals == '0'
              run: |
                  set -euo pipefail
                  echo "Soft-check found 0 meals. Backing off 90s and retrying full scrape..."
                  sleep 90

                  rm -f menu_summary.json consolidated_menu.json
                  rm -f /tmp/menu_summary*.json /tmp/consolidated_menu*.json

                  echo "Retry: Nutrislice..."
                  python -m nutrislice_scraper.main
                  mv menu_summary.json /tmp/menu_summary_nutri.json
                  mv consolidated_menu.json /tmp/consolidated_menu_nutri.json

                  echo "Retry: CBORD..."
                  python -m cbord_scraper.main
                  mv menu_summary.json /tmp/menu_summary_cbord.json
                  mv consolidated_menu.json /tmp/consolidated_menu_cbord.json

                  echo "Retry: merge..."
                  python merge_menus.py                     --cbord /tmp/consolidated_menu_cbord.json                     --nutri /tmp/consolidated_menu_nutri.json                     --out-consolidated /tmp/consolidated_menu.json                     --out-summary /tmp/menu_summary.json

            - name: Validate consolidated_menu.json has meals
              id: validate_menu
              shell: bash
              run: |
                  set -euo pipefail
                  JSON=/tmp/consolidated_menu.json

                  if [[ ! -s "$JSON" ]]; then
                    echo "ERROR: $JSON missing or empty." >&2
                    exit 1
                  fi

                  python - <<'PY'
                  import json, sys
                  p = "/tmp/consolidated_menu.json"
                  with open(p, "r", encoding="utf-8") as f:
                      data = json.load(f)

                  dh = data.get("dining_halls", {})
                  if not isinstance(dh, dict):
                      print("ERROR: dining_halls missing or not a dict", file=sys.stderr)
                      sys.exit(1)

                  total_meals = sum(len(meals) for meals in dh.values() if isinstance(meals, dict))
                  if total_meals == 0:
                      print("ERROR: No meals found across all halls. Failing workflow.", file=sys.stderr)
                      sys.exit(1)

                  print(f"OK: total meals found = {total_meals}")
                  PY

            - name: Upload debug artifacts on failure
              if: failure()
              uses: actions/upload-artifact@v4
              with:
                  name: failed-scrape
                  path: |
                      /tmp/consolidated_menu.json
                      /tmp/menu_summary.json
                      /tmp/consolidated_menu_cbord.json
                      /tmp/menu_summary_cbord.json
                      /tmp/consolidated_menu_nutri.json
                      /tmp/menu_summary_nutri.json
                      **/*.log

            - name: Run script to embed and upsert to Pinecone
              if: success()
              env:
                  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
                  PINECONE_API_KEY: ${{ secrets.PINECONE_API_KEY }}
                  PINECONE_ENV: ${{ secrets.PINECONE_ENV }}
              run: python plate_planner/embed_menu.py /tmp/consolidated_menu.json

            - name: Checkout backend-deployment
              uses: actions/checkout@v3
              with:
                  ref: backend-deployment
                  fetch-depth: 0
                  persist-credentials: true

            - name: Copy JSON from /tmp into backend-deployment root
              run: |
                  cp /tmp/menu_summary.json  ./menu_summary.json
                  cp /tmp/consolidated_menu.json ./consolidated_menu.json

            - name: Commit and push updated menu JSONs
              if: success()
              run: |
                  git add menu_summary.json consolidated_menu.json
                  if ! git diff --cached --quiet; then
                    git config user.name "github-actions[bot]"
                    git config user.email "github-actions[bot]@users.noreply.github.com"
                    git commit -m "ci: daily menu update $(date -u +'%Y-%m-%d')"
                    git push origin backend-deployment
                  else
                    echo "No changes to JSON on backend-deployment, skipping commit."
                  fi
